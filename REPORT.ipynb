{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This repository uses an implementation of the [Deep Deterministic Policy Gradients (DDPG)](https://arxiv.org/abs/1509.02971) algorithm. The implementation heavily borrows from the benchmark implementation in the Deep Reinforcement Learning Nanodegree Program. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradients Algorithm\n",
    "\n",
    "## Elements\n",
    "\n",
    "### Actor\n",
    "* The actor $\\mu:S \\mapsto A$ returns the action $a_t \\in A$ given the state $s_t \\in S$.\n",
    "* In the DDPG algorithm, the actor is approximated by the function $\\mu:S \\times \\Theta_\\mu \\mapsto A$ where\n",
    "   * $\\Theta_\\mu$ is set of parameters of the approximation, in this case the weights of the approximating neural network.\n",
    "* The DDPG algorithm uses 2 neural networks for training purposes:\n",
    "   * $\\theta_\\mu \\in \\Theta_\\mu$: local neural network weights. These are updated at every training step using policy gradients.\n",
    "   * $\\hat{\\theta}_\\mu \\in \\Theta_\\mu$: target neural network weights, these are updated at a slower rate to improve the stability of training.\n",
    "\n",
    "### Critic\n",
    "* The critic is the approximation to the Q-Function and is denoted by $Q:S \\times A \\times \\Theta_{q} \\mapsto \\mathbb{R}$ where\n",
    "  * $\\Theta_\\mu$ is the set of parameters of the neural network used in the approximation.\n",
    "* The DDPG algorithm uses 2 neural networks to for training purposes, hence there are 2 sets of parameters:\n",
    "  * $\\theta_{q} \\in \\Theta_q$ is the set of parameters used by the local neural network.\n",
    "  * $\\hat{\\theta}_{q} \\in \\Theta_q$ is the set of parameters used by the target neural network and are updated at a lower rate to improve the stability of the training. \n",
    "\n",
    "### Noise Process and Randomizations in Actions\n",
    "* The DDPG algorithm uses noise to induce exploration during training. The noise is generated by an [Ornstein - Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) denoted by $N(\\mu, \\theta,\\sigma)$ where \n",
    "  * $\\mu$ is the mean of the noise\n",
    "  * $\\sigma$ is the volatility\n",
    "  * $\\theta$ is the decay-rate.\n",
    "* The DDPG algorithm computes the action values as follows\n",
    "  \\begin{align*}\n",
    "   a_t     &= \\mu(s_t, \\theta_\\mu) + \\epsilon_t N(\\mu, \\theta, \\sigma)\\\\\n",
    "   e_{t+1} &= \\gamma_\\epsilon \\epsilon_t \n",
    "  \\end{align*}\n",
    "  where the randomness rate,$\\epsilon_t$, is between [0,1] and $\\gamma_\\epsilon \\in (0,1)$ is the decay rate of $\\epsilon_t$ and the initial randomness rate $\\epsilon_0$. \n",
    "* Each element of the action vector is clipped so that they are between -1 and 1:\n",
    "  \\begin{align*}\n",
    "   a_t[k] \\leftarrow \\max(\\min(1, a_t[k]), -1)\n",
    "  \\end{align*}\n",
    "\n",
    "### The Experience Replay Buffer \n",
    "* DDPG stores the experience $(s_t, a_t, r_{t+1}, s_{t+1})$ where \n",
    "  * $s_t$ is the state at time $t$\n",
    "  * $s_{t+1}$ is the state at time $t+1$\n",
    "  * $a_t$ is the action at time $t$\n",
    "  * $r_{t+1}$ is the reward obtained at time $t+1$, \n",
    "in a buffer of fixed length $N_B$.\n",
    "* The stored experiences are used in neural network training.  The experiences are used in batches of size $N_s$. \n",
    "\n",
    "### Critic and Actor Training\n",
    "* At every $T$ time steps, the local critic neural network is trained as follows:\n",
    "     1. For $K$ update steps repeat:\n",
    "        1. Select a batch of size $N_s$ experiences randomly from the replay buffer.\n",
    "        2. For each experience , compute:\n",
    "           \\begin{align*}\n",
    "           y_i &= r_{t+1,i} + \\gamma Q(s_{t+1,i}, \\mu(s_{t+1,i},\\hat{\\theta}_\\mu),\\hat{\\theta}_q)\\;\\; \\forall i \\in \\{1, \\dots, N_s\\}\n",
    "           \\end{align*}\n",
    "           where $\\gamma$ is the discount factor.\n",
    "        3. Update $\\theta_q$ as follows:\n",
    "           \\begin{align*}\n",
    "           \\theta_q &\\leftarrow \\theta_q + \\frac{\\alpha_q}{N_s} \\sum_{i=1}^{N_s}(y_i- Q(s_{t,i},a_{t,i}, \\theta_q))\\nabla_{\\theta_q} Q(s_{t,i},a_{t,i}, \\theta_q)\n",
    "           \\end{align*}\n",
    "           to minimize the objective function\n",
    "           \\begin{align*}\n",
    "            L = \\frac{1}{N_s}\\sum_{i =1}^{N_s} (y_i- Q(s_{t,i},a_{t,i}, \\theta_q))^2 \n",
    "           \\end{align*} \n",
    "           where $\\alpha_q$ is the critic learning rate.       \n",
    "* The local actor neural network weights are updated using a policy gradient approach.\n",
    "* At every $T$ time steps, the local critic neural network is trained as follows:\n",
    "     1. For $K$ update steps repeat:\n",
    "        1. Select a batch of size $N_s$ experiences randomly from the replay buffer.\n",
    "        2. Compute the policy gradient and update the weights.\n",
    "        \\begin{align*}\n",
    "            \\nabla J_{\\theta_{\\mu}} &= \\frac{1}{N_s}\\sum_{i=1}^{N_s}\\nabla Q_{a}(s_{t,i},a_{t,i}, \\hat{\\theta}_q)\\nabla_{\\theta_\\mu}\\mu(s_{t,i}, \\theta_\\mu) \\\\\n",
    "            \\theta_\\mu &\\leftarrow \\theta_\\mu + \\alpha_\\mu \\nabla J_{\\theta_{\\mu}}\n",
    "        \\end{align*}\n",
    "        where $\\alpha_\\mu$ is the learning rate for the actor. \n",
    "\n",
    "### Continuous Target Actor and Critic Update\n",
    "* The target actor and critic neural network weights are updated gradually at every time $T$ time steps and every time the local counterparts are updated according the following rules: \n",
    " \\begin{align*}\n",
    "   \\hat{\\theta}_q &\\leftarrow  (1-\\tau)\\hat{\\theta}_q + \\tau\\theta_q \\\\\n",
    "   \\hat{\\theta}_{\\mu} &\\leftarrow  (1-\\tau)\\hat{\\theta}_{\\mu} + \\tau\\theta_{\\mu}\n",
    "  \\end{align*}\n",
    "where $\\tau$ is the soft update parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Algorithm\n",
    "0. Initialize $\\theta_\\mu$ and $\\theta_q$ randomly. \n",
    "   * Set $\\hat{\\theta}_\\mu = \\hat{\\theta}$ and $\\hat{\\theta}_q = \\theta_q$. \n",
    "   * Set the parameters $\\tau$,$\\gamma$, $\\alpha_\\mu$, $\\alpha_q$, $N_B$, $N_s$, $\\mu$, $\\theta$, $\\sigma$, $\\epsilon_0$, $\\gamma_\\epsilon$, $T$, $K$  \n",
    "   \n",
    "1. for $episode =1, \\;\\; M$ do\n",
    "   1. Initialize random process $N(\\mu,\\theta,\\sigma)$.\n",
    "   2. Receive initial state $s_1$\n",
    "   3. for $t =1, \\;t_{final}$ do\n",
    "      1. compute $a_t$ as  \n",
    "          \\begin{align*}\n",
    "            a_t     &= \\mu(s_t, \\theta_\\mu) + \\epsilon_t N(\\mu, \\theta, \\sigma)\\\\\n",
    "            a_t[k]  &\\leftarrow \\max(\\min(1, a_t[k]), -1)\\\\\n",
    "            e_{t+1} &= \\gamma_\\epsilon \\epsilon_t \n",
    "          \\end{align*}\n",
    "      2. Execute $a_t$, receive $r_{t=1}$ and transition to new state $s_{t+1}$.\n",
    "      3. Store experience $(s_t, a_t, r_{t+1}, s_{t+1})$ in the experience replay buffer.\n",
    "      4. If $t\\bmod T = 0$ and the number of experiences stored in the replay buffer is greater than $N_s$\n",
    "         1. for $u = 1,\\; K$ do\n",
    "            1. Obtain a batch of experiences from the replay buffer of size $N_s$.\n",
    "            2. Update the critic local network weights. \n",
    "                 * For each experience , compute:\n",
    "                   \\begin{align*}\n",
    "                     y_i &= r_{t+1,i} + \\gamma Q(s_{t+1,i}, \\mu(s_{t+1,i},\\hat{\\theta}_\\mu),\\hat{\\theta}_q)\\;\\; \\forall i \\in \\{1, \\dots, N_s\\}\n",
    "                    \\end{align*}\n",
    "                 * Update $\\theta_q$ as follows:\n",
    "                   \\begin{align*}\n",
    "                      \\theta_q &\\leftarrow \\theta_q + \\frac{\\alpha_q}{N_s} \\sum_{i=1}^{N_s}(y_i- Q(s_{t,i},a_{t,i}, \\theta_q))\\nabla_{\\theta_q} Q(s_{t,i},a_{t,i}, \\theta_q)\n",
    "                    \\end{align*}\n",
    "            3. Update the actor local network weights using the policy gradient:   \n",
    "                \\begin{align*}\n",
    "                   \\nabla J_{\\theta_{\\mu}} &= \\frac{1}{N_s}\\sum_{i=1}^{N_s}\\nabla Q_{a}(s_{t,i},a_{t,i}, \\hat{\\theta}_q)\\nabla_{\\theta_\\mu}\\mu(s_{t,i}, \\theta_\\mu) \\\\\n",
    "                   \\theta_\\mu &\\leftarrow \\theta_\\mu + \\alpha_\\mu \\nabla J_{\\theta_{\\mu}}\n",
    "                \\end{align*}\n",
    "            4. Soft update the critic target and actor target neural network weights.   \n",
    "                \\begin{align*}\n",
    "                   \\hat{\\theta}_q &\\leftarrow  (1-\\tau)\\hat{\\theta}_q + \\tau\\theta_q \\\\\n",
    "                   \\hat{\\theta}_{\\mu} &\\leftarrow  (1-\\tau)\\hat{\\theta}_{\\mu} + \\tau\\theta_{\\mu}\n",
    "                \\end{align*} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Details\n",
    "## Neural Network Structure\n",
    "\n",
    "### Actor Target and Local Neural Networks\n",
    "\n",
    "* The structure is as follows for both the actor local and actor target neural networks:\n",
    "    1. Layer: Input Layer(input_dim = size of the state vector, output_dim = 256)\n",
    "    2. Normalization: Batch normalization(input_dim = 256, output_dim = 256)\n",
    "    3. Activation: Relu Activation\n",
    "    4. Layer: Hidden Layer(input_dim = 256, output_dim = 256)\n",
    "    5. Activation: Relu Activation\n",
    "    6. Layer: Hidden Layer(input_dim = 256, output_dim = size of the action vector)\n",
    "    7. Activation: Tanh\n",
    "    \n",
    "### Critic Target and Local Neural Networks\n",
    "\n",
    "* The structure is as follows for both the \n",
    "    1. Layer: Input Layer (input_dim = size of the state vector,output_dim = 256)\n",
    "    2. Normalization: Batch Normalization\n",
    "    3. Activation: Relu\n",
    "    4. Layer: Hidden (input_dim = 256 + size of the action vector,  output_dim = 256)\n",
    "    5. Activation: Relu\n",
    "    6. Layer: Output Layer(input_dim = 256, output_dim = 1)\n",
    "    \n",
    "## Hyperparameters\n",
    "\n",
    "* Soft update parameter,$\\tau$: 0.001\n",
    "* Discount factor, $\\gamma$:0.99\n",
    "* Learning rate for the actor local training, $\\alpha_\\mu$:0.00005\n",
    "* Learning rate for the critic local training, $\\alpha_q$:0.00005\n",
    "* Replay buffer size, $N_B$: 1,000,000\n",
    "* Training batch size $N_s$: 128\n",
    "* Ornstein-Uhlenbeck mean, $\\mu$: 0.0\n",
    "* Ornstein-Uhlenbeck decay rate, $\\theta$:0.15\n",
    "* Ornstein-Uhlenbeck volatility, $\\sigma$:0.2\n",
    "* Initial randomness rate, $\\epsilon_0$: 1.0\n",
    "* Randomness rate decay rate, $\\gamma_\\epsilon$: 0.999\n",
    "* Frequency of update for the neural networks, $T$: 5\n",
    "* Number of steps taken at every update, $K$: 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training Results\n",
    " The agent achieves an average score of 13 or above in about 113 episodes.\n",
    "* The progression of the average of the  scores of all agents and the moving average score over 100 episodes is shown in the next figure.\n",
    "* `Moving Average Score over 100 Episodes` is the moving average score and `Average Score` is the average of the scores at each episode. \n",
    "![Training Performance](training_performance.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Directions\n",
    "* The hyper parameters for the algorithm have been chosen by trial and error. Better parameters can be found by using an automated hyperparameter search techniques.\n",
    "* [Prioritized experienced replay](https://arxiv.org/abs/1511.05952) can improve the performance with less\tincrease in training time. Here, the training samples are given priorities based on\ttheir TD error.\n",
    "* Experiment with  other algorithms:\n",
    "   * [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)\n",
    "   * [D4PG](https://openreview.net/pdf?id=SyZipzbCb):  This algorithm includes prioritized experience replay and is shown to achieve higher performance quicker than DDPG. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
