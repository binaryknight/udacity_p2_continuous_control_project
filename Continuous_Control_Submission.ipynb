{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Continuous Control Project Submission\n",
    "\n",
    "* In this notebook, a solution for the second project of the Udacity Reinforcement Learning Course is shown. \n",
    "* The program trains an agent using the Reacher environment with 20 robots.\n",
    "* The top level functionality is implemented in `continuous_control.py`\n",
    "* The `src` directory contains the following files:\n",
    "  * `ddpg_agent.py`: This file implements the  Deep Deterministic Policy Gradient algorithm (DDPG).\n",
    "     * This implementation is heavily based on the Udacity Reinforcement Learning Course implementation   \n",
    "  * `model.py`: This file implements the 4 neural networks of the  (DDPG) algorithm.\n",
    "     * This implementation is heavily based on the Udacity Reinforcement Learning Course implementation \n",
    "* The `README.md` file contains setup details\n",
    "* The `Report.ipynb` file contains the description of the algorithm and other implementation details.\n",
    "* The weights of the trained neural networks are in the following files:\n",
    "  * `actor_local_weights.pt`\n",
    "  * `actor_target_weights.pt`\n",
    "  * `critic_local_weights.pt`\n",
    "  * `critic_target_weights.pt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import continuous_control as cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the simulaton environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    " env = UnityEnvironment(file_name=\"Reacher20.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Number of actions: 4\n",
      "States have length: 33\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "Episode: 1 Episode Duration: 174s min_score: 0.00 max_score: 1.26 average_score: 0.42\n",
      "Episode: 2 Episode Duration: 182s min_score: 0.15 max_score: 2.04 average_score: 0.63\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "num_episodes,  avg_scores, scores = cc.train(env, num_episodes=2)\n",
    "print(\"Done training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = [np.mean(m) for m in scores]\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "_ = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(mean_scores)), mean_scores)\n",
    "plt.plot(np.arange(len(avg_scores)), avg_scores)\n",
    "plt.ylabel('Score and Average Scores ')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(['Score', 'Average Score'], loc='upper left')\n",
    "plt.savefig('training_performance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run the trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cc.run(env,num_episodes=1,actor_local_load_filename='actor_local_weights.pt')\n",
    "print(\"Done simulating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Close the  simulation environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
